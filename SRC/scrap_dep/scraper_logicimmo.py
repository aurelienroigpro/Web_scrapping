import re
import time
import random
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# ‚öôÔ∏è PARAM√àTRES
# ============================================================

INPUT_CSV = "DATA/departements.csv"
OUTPUT_CSV = "annonce.csv"

START_DEPARTEMENT_CODE = None
NB_DEPARTEMENTS_A_SCRAPER = None     # Mets None pour tout faire (apr√®s test)
NB_PAGES_PAR_DEPARTEMENT = 4      # Max pages √† tenter (stop si page vide)

# Pauses (anti-blocage ‚Äúsoft‚Äù)
SLEEP_BETWEEN_PAGES = (4.1, 6.6)      # secondes
SLEEP_BETWEEN_DEPS = (10, 20)     # secondes


# ============================================================
# üç™ Cookies (Usercentrics)
# ============================================================

def accept_cookies_if_present(driver, timeout=8):
    """
    Clique sur "Tout accepter" si la popup cookies est affich√©e.
    Ne fait rien si elle n'est pas pr√©sente.
    """
    try:
        wait = WebDriverWait(driver, timeout)

        # Bouton "Tout accepter"
        btn = wait.until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'Tout accepter')]"))
        )
        btn.click()
        time.sleep(1.5)
        print("      ‚úÖ Cookies accept√©s")
    except Exception:
        # Pas de popup, ou d√©j√† accept√©
        pass


# ============================================================
# üß† Parsing ALT
# ============================================================

def parse_from_alt(alt: str) -> dict | None:
    """
    Retourne un dict si l'annonce correspond √† nos types + prix + surface + adresse,
    sinon None.
    """

    # Type + sous-type
    if "Appartement √† vendre" in alt:
        type_bien, sous_type = "Appartement", "Appartement"
    elif "Duplex √† vendre" in alt:
        type_bien, sous_type = "Appartement", "Duplex"
    elif "Maison √† vendre" in alt:
        type_bien, sous_type = "Maison", "Maison"
    elif "Pavillon √† vendre" in alt:
        type_bien, sous_type = "Maison", "Pavillon"
    else:
        return None

    # Prix
    m_price = re.search(r"(\d[\d\s ]+)\s*‚Ç¨", alt)  # inclut l'espace fine ins√©cable ( )
    # Surface (pas terrain)
    m_surface = re.search(r"(\d[\d\s,]+)\s*m¬≤(?!\s*de terrain)", alt)
    # Pi√®ces
    m_pieces = re.search(r"(\d+)\s+pi√®ce", alt, flags=re.IGNORECASE)
    # Adresse ville + CP (g√®re Lyon 3√®me 69003 / LYON 3EME, 69003)
    m_addr = re.findall(r"([A-Z√Ä-√ù][A-Za-z0-9√Ä-√ø\-' ]+)\s*,?\s*\(?(\d{5})\)?", alt)

    if not (m_price and m_surface and m_addr):
        return None

    ville, cp = m_addr[-1]

    return {
        "prix": m_price.group(1).strip(),
        "surface": m_surface.group(1).strip(),
        "pieces": m_pieces.group(1) if m_pieces else None,
        "adresse": f"{ville.strip()} ({cp})",
        "type_bien": type_bien,
        "sous_type": sous_type,
    }


# ============================================================
# üîó URL pagination
# ============================================================

def build_page_url(base_url: str, page: int) -> str:
    if page == 1:
        return base_url

    m = re.search(r"(ad\d+\w+)$", base_url.lower())
    if not m:
        raise ValueError(f"Code AD introuvable dans l'URL : {base_url}")

    ad_code = m.group(1).upper()

    return (
        "https://www.logic-immo.com/classified-search"
        f"?distributionTypes=Buy&locations={ad_code}&page={page}&order=DateDesc"
    )


# ============================================================
# üß≠ D√©partement (nom + code)
# ============================================================

def parse_dep(nom: str):
    m = re.search(r"\((\d+|2a|2b)\)", nom.lower())
    dep_code = m.group(1).upper() if m else None
    dep_nom = nom.replace("Immobilier", "").split("(")[0].strip()
    return dep_nom, dep_code


# ============================================================
# üåç Scraping d'un d√©partement
# ============================================================

def collect_ads_for_department(driver, base_url: str) -> list:
    ads = []
    cookies_checked = False

    for page in range(1, NB_PAGES_PAR_DEPARTEMENT + 1):
        url = build_page_url(base_url, page)
        print(f"  ‚Üí Page {page} : {url}")

        driver.get(url)
        time.sleep(2.5)

        # On g√®re la popup cookies une seule fois au d√©but du run (ou au besoin)
        if not cookies_checked:
            accept_cookies_if_present(driver)
            cookies_checked = True
            time.sleep(1.5)

        time.sleep(random.uniform(*SLEEP_BETWEEN_PAGES))

        soup = BeautifulSoup(driver.page_source, "html.parser")

        imgs = soup.find_all("img", alt=True)
        count_page = 0

        for img in imgs:
            data = parse_from_alt(img.get("alt", ""))
            if data:
                ads.append(data)
                count_page += 1

        print(f"      Annonces valides sur cette page : {count_page}")

        # Si une page ne retourne rien, on suppose fin de pagination / page bloqu√©e
        if count_page == 0:
            break

    return ads


# ============================================================
# üöÄ MAIN
# ============================================================

def main():
    df = pd.read_csv(INPUT_CSV)
    df["dep_code"] = df["nom"].apply(lambda x: parse_dep(str(x))[1])

    # Reprise √† partir d'un code
    if START_DEPARTEMENT_CODE:
        start_code = START_DEPARTEMENT_CODE.upper()
        if start_code not in df["dep_code"].values:
            raise ValueError(f"START_DEPARTEMENT_CODE='{START_DEPARTEMENT_CODE}' introuvable dans le CSV.")
        start_idx = df.index[df["dep_code"] == start_code][0]
        df = df.loc[start_idx:].reset_index(drop=True)
        print(f"Reprise √† partir du d√©partement {start_code} (ligne CSV originale {start_idx}).\n")

    if NB_DEPARTEMENTS_A_SCRAPER is not None:
        df = df.head(NB_DEPARTEMENTS_A_SCRAPER)

    print("D√©partements √† scraper :", len(df))

    # Firefox (non-headless)
    options = webdriver.FirefoxOptions()
    # options.add_argument("--headless")  # laisse comment√©

    driver = webdriver.Firefox(
        service=Service(GeckoDriverManager().install()),
        options=options
    )

    all_ads = []

    for idx, row in df.iterrows():
        dep_nom, dep_code = parse_dep(row["nom"])
        print(f"\n=== [{idx+1}/{len(df)}] D√©partement {dep_code} ‚Äì {dep_nom} ===")
        print("URL de base :", row["url"])

        ads = collect_ads_for_department(driver, row["url"])
        print(f"  ‚Üí Total annonces valides pour ce d√©partement : {len(ads)}")

        for ad in ads:
            ad["departement_nom"] = dep_nom
            ad["departement_code"] = dep_code
            all_ads.append(ad)

        time.sleep(random.uniform(*SLEEP_BETWEEN_DEPS))

    driver.quit()
    print("\nNavigateur ferm√©.")

    df_final = pd.DataFrame(all_ads)
    print("\nTotal annonces r√©colt√©es :", len(df_final))

    df_final.to_csv(OUTPUT_CSV, index=False, encoding="utf-8")
    print("CSV final cr√©√© ‚Üí", OUTPUT_CSV)


if __name__ == "__main__":
    main()
